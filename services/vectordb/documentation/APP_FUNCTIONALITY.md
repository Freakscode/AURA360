# ğŸ“š AURA365 Vectorial Database - Complete Functionality Overview

## ğŸ¯ Executive Summary

**AURA365 Vectorial Database** is a production-ready, scalable service for semantic search and retrieval of biomedical documents. The system provides end-to-end capabilities for downloading, processing, vectorizing, and searching scientific literature with sub-100ms latency and 90% cache efficiency.

---

## ğŸ—ï¸ System Architecture

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER INTERFACE                            â”‚
â”‚                    (REST API / CLI Scripts)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â–¼               â–¼               â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  FastAPI  â”‚   â”‚  Celery   â”‚   â”‚  Scripts  â”‚
        â”‚    API    â”‚â”€â”€â”€â”‚  Workers  â”‚   â”‚ (CLI/Bat) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚              â”‚               â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Redis (Cache)  â”‚
                    â”‚ + Message Queue â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                      â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Qdrant     â”‚      â”‚    GROBID     â”‚     â”‚  External    â”‚
â”‚ Vector Store  â”‚      â”‚  PDF Parser   â”‚     â”‚  Sources     â”‚
â”‚  (Vectors)    â”‚      â”‚  (Metadata)   â”‚     â”‚ (Sci-Hub/GCS)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **API** | FastAPI 0.118+ | REST endpoints, async I/O |
| **Workers** | Celery 5.5+ | Distributed task processing |
| **Vector DB** | Qdrant 1.15+ | Vector storage & search |
| **Cache** | Redis 6.4+ | Embeddings cache & message broker |
| **Parser** | GROBID + PyMuPDF | PDF extraction & metadata |
| **Embeddings** | FastEmbed 0.7+ | MiniLM-L12-v2 (384-dim) |
| **Orchestration** | Docker Compose | Multi-service deployment |

---

## ğŸš€ Core Functionalities

### 1. Document Acquisition (Sci-Hub Downloader)

**Script**: `scripts/download_scihub_batch.py`

#### Capabilities
- **Multi-format support**: DOI, PMID, arXiv, URLs
- **Intelligent routing**: Automatically selects best Sci-Hub mirror
- **Fault tolerance**: 5 mirrors with automatic fallback
- **Retry logic**: Exponential backoff (3 attempts per mirror)
- **Content validation**: Verifies PDF integrity
- **Rate limiting**: 3-5 second delays to avoid blocking
- **Batch processing**: Process hundreds of papers
- **Progress tracking**: Real-time download status
- **Auto-ingestion**: Optional direct upload to vectorial system

#### Usage Example
```bash
# Download from DOI list
python3 scripts/download_scihub_batch.py \
  --input dois.txt \
  --output downloads/papers \
  --max-workers 3 \
  --delay 5

# With auto-ingestion
python3 scripts/download_scihub_batch.py \
  --input dois.txt \
  --output downloads/papers \
  --auto-ingest \
  --api-url http://localhost:8000
```

#### Supported Identifiers
```python
# DOI formats
10.1038/nature12373
doi:10.1126/science.1259855
https://doi.org/10.1016/j.cell.2020.04.001

# PubMed IDs
PMID:12345678
pmid:98765432

# Direct URLs
https://sci-hub.se/10.1038/nature12373
```

#### Features
âœ… **5 Sci-Hub mirrors** with auto-fallback  
âœ… **Content validation** (PDF magic bytes check)  
âœ… **Parallel downloads** (configurable workers)  
âœ… **Resume capability** (skip existing files)  
âœ… **Comprehensive logging** (success/failure tracking)  
âœ… **Error reporting** (failed_downloads.txt)  

---

### 2. Document Processing Pipeline

**Core**: `vectosvc/core/pipeline.py`

#### Processing Stages

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Downloadâ”‚â”€â”€â”€â–¶â”‚  Parse  â”‚â”€â”€â”€â–¶â”‚  Chunk  â”‚â”€â”€â”€â–¶â”‚  Embed  â”‚â”€â”€â”€â–¶â”‚  Store  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚              â”‚              â”‚              â”‚              â”‚
     â–¼              â–¼              â–¼              â–¼              â–¼
  FS/HTTP/     GROBID +        Smart         FastEmbed       Qdrant
    GCS        PyMuPDF      Segmentation      +Cache        +Metadata
```

#### Stage 1: Download Phase
- **Sources**: Local filesystem, HTTP/HTTPS, Google Cloud Storage
- **Validation**: File existence, size checks, content type
- **Error handling**: Retry with exponential backoff
- **Performance**: Connection pooling, async downloads

#### Stage 2: Parse Phase
- **Primary**: GROBID (scientific PDF parser)
  - Extracts: Title, authors, abstract, sections, references
  - Identifies: Header, body, conclusion sections
  - Preserves: Document structure and hierarchy
- **Fallback**: PyMuPDF (text extraction)
  - Used when GROBID fails or times out
  - Fast, reliable text extraction
- **Metadata extraction**:
  - Title, authors, DOI, journal, year
  - Publication metadata
  - Language detection (langdetect)

#### Stage 3: Chunk Phase
- **Strategy**: Section-aware chunking
- **Parameters**:
  - Chunk size: 900 tokens (configurable)
  - Overlap: 150 tokens (configurable)
  - Consolidate citations: Optional
- **Metadata per chunk**:
  ```python
  {
    "doc_id": str,
    "chunk_index": int,
    "text": str,
    "section_path": str,  # e.g., "Introduction/Background"
    "section_order": int,
    "is_abstract": bool,
    "is_conclusion": bool,
    "lang": str,  # ISO language code
    "title": str,
    "authors": List[str],
    "doi": str,
    "journal": str,
    "year": int,
    "topics": List[str],  # Auto-classified
    "metadata": Dict  # User metadata
  }
  ```

#### Stage 4: Embed Phase
- **Model**: paraphrase-multilingual-MiniLM-L12-v2
- **Dimension**: 384
- **Cache**: Redis with 7-day TTL
- **Performance**: 
  - Cold start: ~11.13s per document
  - Cache hit: ~1.12s per document (90% improvement)
- **Batch processing**: Efficient batching for multiple chunks

#### Stage 5: Store Phase
- **Vector storage**: Qdrant with Cosine distance
- **Indexing**: Automatic payload indexing for fast filtering
- **Metadata**: Full JSON payload attached to each vector
- **Deduplication**: Hash-based to prevent duplicates
- **Upsert logic**: Update existing documents

#### Stage 6: Topic Classification (Optional)
- **Categories**: 37 biomedical topics
- **Method**: Keyword-based classification from YAML config
- **Topics include**:
  - Sleep/Circadian Rhythms
  - Obesity/Weight Management
  - Metabolic Syndrome
  - Cardiovascular Health
  - Diabetes/Insulin Resistance
  - And 32 more...

---

### 3. Batch Ingestion

**Script**: `scripts/ingest_batch.py`

#### Capabilities
- **Directory scanning**: Recursive PDF discovery
- **Parallel submission**: Multiple concurrent API requests
- **Progress monitoring**: Real-time job status tracking
- **Error handling**: Automatic retry with DLQ
- **Metadata tagging**: Batch-level metadata
- **Docker compatibility**: Smart path resolution

#### Usage Example
```bash
# Basic ingestion
python3 scripts/ingest_batch.py \
  --directory downloads/papers \
  --api-url http://localhost:8000

# With metadata and monitoring
python3 scripts/ingest_batch.py \
  --directory downloads/papers \
  --api-url http://localhost:8000 \
  --metadata '{"project":"obesity","source":"scihub","batch":"2024-10"}' \
  --monitor

# Without monitoring (faster)
python3 scripts/ingest_batch.py \
  --directory downloads/papers \
  --api-url http://localhost:8000 \
  --no-monitor
```

#### Features
âœ… **Auto-discovery** of PDFs in directory  
âœ… **Batch metadata** applied to all documents  
âœ… **Job monitoring** with progress bar  
âœ… **Error reporting** with job IDs  
âœ… **Resume capability** (skip processed)  
âœ… **Docker-aware** path handling  

---

### 4. REST API

**Service**: `vectosvc/api/http.py` (Port 8000)

#### Endpoints

##### POST `/ingest`
Submit a single document for processing.

**Request**:
```json
{
  "doc_id": "paper-001",
  "url": "https://example.com/paper.pdf",
  "filename": "paper.pdf",
  "metadata": {
    "project": "research",
    "year": 2024
  },
  "chunk_size": 900,
  "chunk_overlap": 150
}
```

**Response**:
```json
{
  "job_id": "abc123...",
  "status": "queued"
}
```

##### POST `/ingest/batch`
Submit multiple documents.

**Request**:
```json
{
  "requests": [
    {"doc_id": "doc1", "url": "file://path/to/doc1.pdf"},
    {"doc_id": "doc2", "url": "gs://bucket/doc2.pdf"}
  ]
}
```

**Response**:
```json
{
  "job_ids": ["job1", "job2"],
  "submitted": 2,
  "failed": 0
}
```

##### POST `/api/v1/holistic/search`
Primary endpoint consumido por el servicio de agentes con trazabilidad y metadatos enriquecidos.

**Request**:
```json
{
  "trace_id": "trace-123",
  "query": "beneficios de la meditaciÃ³n en ansiedad",
  "category": "mente",
  "locale": "es-CO",
  "top_k": 5,
  "embedding_model": "text-embedding-3-small",
  "filters": {
    "must": {
      "source_type": "paper"
    },
    "should": {
      "topics": ["mindfulness"]
    }
  }
}
```

**Response**:
```json
{
  "status": "success",
  "data": {
    "results": [
      {
        "document_id": "doc-001",
        "score": 0.83,
        "confidence_score": 0.79,
        "category": "mente",
        "source_type": "paper",
        "chunk": "La meditaciÃ³n guiada redujo los niveles de ansiedad...",
        "metadata": {
          "title": "MeditaciÃ³n y salud mental",
          "year": 2024,
          "section_path": "results"
        },
        "embedding_model": "text-embedding-3-small",
        "version": "2025.10.27"
      }
    ]
  },
  "error": null,
  "meta": {
    "trace_id": "trace-123",
    "took_ms": 112,
    "collection": "holistic_memory"
  }
}
```

Errores controlados devuelven `"status": "error"` con `error.type`, `error.message`, detalles y cÃ³digo HTTP (400/502/500) consistente con la causa.

##### POST `/search`
Semantic search with filtering.

**Request**:
```json
{
  "query": "sleep duration and obesity relationship",
  "limit": 10,
  "score_threshold": 0.7,
  "filter": {
    "must": {
      "year": {"$gte": 2020}
    },
    "should": {
      "topics": "obesity"
    }
  }
}
```

**Response**:
```json
{
  "hits": [
    {
      "id": "chunk-uuid",
      "score": 0.89,
      "payload": {
        "doc_id": "paper-001",
        "title": "Sleep Duration and Obesity...",
        "text": "Our study found that...",
        "year": 2023,
        "authors": ["Smith J", "Doe A"]
      }
    }
  ],
  "total": 1,
  "query_time_ms": 45.2
}
```

##### GET `/jobs/{job_id}`
Check processing status.

**Response**:
```json
{
  "job_id": "abc123",
  "status": "completed",  // queued, processing, completed, failed
  "doc_id": "paper-001",
  "processed_chunks": 45,
  "total_chunks": 45,
  "error": null
}
```

##### GET `/metrics`
System statistics and metrics.

**Response**:
```json
{
  "service": {
    "name": "vectosvc",
    "version": "0.1.0",
    "uptime_seconds": 3600
  },
  "collection": {
    "name": "docs",
    "total_points": 1228,
    "total_documents": 26,
    "vector_size": 384,
    "distance_metric": "Cosine"
  },
  "cache": {
    "embeddings_enabled": true,
    "hits": 450,
    "misses": 50,
    "hit_rate_percent": 90.0
  },
  "pipeline": {
    "total_documents": 26,
    "total_chunks": 1228,
    "total_errors": 0
  }
}
```

##### GET `/dlq`
View failed jobs (Dead Letter Queue).

**Response**:
```json
{
  "entries": [
    {
      "job_id": "failed-job-1",
      "doc_id": "problem-doc",
      "error": "GROBID timeout",
      "error_type": "TimeoutError",
      "traceback": "...",
      "attempts": 4,
      "failed_at": 1234567890,
      "error_phase": "parse"
    }
  ],
  "stats": {
    "total_failures": 1,
    "by_phase": {"parse": 1},
    "by_error": {"TimeoutError": 1}
  }
}
```

##### DELETE `/dlq`
Clear the Dead Letter Queue.

---

### 5. Search Capabilities

#### Semantic Search
- **Model**: Multilingual embeddings (100+ languages)
- **Algorithm**: Cosine similarity on 384-dimensional vectors
- **Latency**: Sub-100ms typical, ~50ms average
- **Scalability**: Efficient for millions of chunks

#### Advanced Filtering

**Supported Operators**:
```python
{
  "must": {...},        # AND logic
  "should": {...},      # OR logic
  "must_not": {...}     # NOT logic
}
```

**Field Filters**:
```python
{
  "year": {"$gte": 2020, "$lte": 2024},
  "topics": "obesity",
  "is_conclusion": true,
  "journal": "Nature",
  "authors": {"$in": ["Smith J", "Doe A"]}
}
```

**Example Queries**:

```bash
# Find recent papers about sleep
curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "sleep deprivation effects",
    "limit": 5,
    "filter": {"must": {"year": {"$gte": 2022}}}
  }'

# Find conclusions about metabolic syndrome
curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "metabolic syndrome treatment",
    "limit": 10,
    "filter": {"must": {"is_conclusion": true}}
  }'

# Multi-topic search
curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "cardiovascular risk factors",
    "filter": {
      "should": {
        "topics": ["cardiovascular", "diabetes"]
      }
    }
  }'
```

---

### 6. Caching System

**Technology**: Redis with intelligent TTL

#### Embedding Cache
- **Purpose**: Avoid re-computing embeddings for identical text
- **Strategy**: Hash-based key generation
- **TTL**: 7 days (604,800 seconds)
- **Eviction**: LRU (Least Recently Used)
- **Performance**: 90% hit rate in production

#### Cache Statistics
```bash
# View cache metrics
curl http://localhost:8000/metrics | jq .cache

# Output:
{
  "embeddings_enabled": true,
  "embeddings_ttl_seconds": 604800,
  "hits": 450,
  "misses": 50,
  "total": 500,
  "hit_rate_percent": 90.0
}
```

---

### 7. Error Handling & Reliability

#### Dead Letter Queue (DLQ)
- **Purpose**: Track and analyze failed jobs
- **Retention**: Persistent until manually cleared
- **Information captured**:
  - Full request payload
  - Error message and type
  - Complete stack trace
  - Failure timestamp
  - Number of retry attempts
  - Phase where failure occurred

#### Retry Logic
- **Strategy**: Exponential backoff
- **Attempts**: 4 tries per job
- **Backoff**: 10s, 30s, 90s, 270s
- **Phases retried**: All (download, parse, chunk, embed, store)

#### Monitoring
```bash
# Check DLQ statistics
curl http://localhost:8000/dlq/stats

# View failed jobs
curl http://localhost:8000/dlq | jq

# Clear DLQ after analysis
curl -X DELETE http://localhost:8000/dlq
```

---

### 8. Multi-Source Support

#### Local Filesystem
```python
{
  "url": "downloads/papers/paper.pdf",  # Relative path
  "url": "file:///absolute/path/paper.pdf"  # Absolute path
}
```

#### HTTP/HTTPS
```python
{
  "url": "https://example.com/paper.pdf"
}
```
- Timeout: 30 seconds
- Retry: 3 attempts
- User-Agent: Custom for compliance

#### Google Cloud Storage
```python
{
  "url": "gs://bucket-name/path/to/paper.pdf"
}
```
- **Authentication**: Service account or ADC
- **Requester-pays**: Optional support
- **Performance**: Streaming downloads

---

### 9. Topic Classification

**Configuration**: `config/topics.yaml`

#### 37 Biomedical Categories

1. **Sleep & Circadian**
   - Sleep duration, quality, disorders
   - Circadian rhythms, chronotype
   - Sleep apnea, insomnia

2. **Obesity & Weight**
   - BMI, body composition
   - Weight gain/loss
   - Bariatric surgery

3. **Metabolic Syndrome**
   - Insulin resistance
   - Hypertension
   - Dyslipidemia

4. **Nutrition**
   - Diet patterns
   - Macronutrients
   - Nutritional interventions

5. **Cardiovascular**
   - Heart disease
   - Blood pressure
   - Atherosclerosis

... and 32 more categories

#### Auto-Classification
- **Method**: Keyword matching from YAML
- **Multiple topics**: Documents can have multiple classifications
- **Searchable**: Filter by topic in queries
- **Extensible**: Add new topics by editing YAML

---

### 10. Docker Deployment

**Orchestration**: `docker-compose.yml`

#### Services

**1. qdrant** (Vector Database)
```yaml
Ports: 6333 (HTTP), 6334 (gRPC)
Volume: ./qdrant_storage:/qdrant/storage
Image: qdrant/qdrant:v1.11.3
```

**2. redis** (Cache & Broker)
```yaml
Port: 6379
Config: maxmemory 512mb, allkeys-lru
Image: redis:7-alpine
```

**3. grobid** (PDF Parser)
```yaml
Port: 8070
Image: lfoppiano/grobid:0.8.1
```

**4. api** (FastAPI)
```yaml
Port: 8000
Depends: qdrant, redis, grobid
Volumes: .:/app:ro
```

**5. worker** (Celery)
```yaml
Concurrency: 2 workers
Depends: qdrant, redis, grobid
Volumes: .:/app:ro
```

#### Deployment Commands
```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f api worker

# Scale workers
docker-compose up -d --scale worker=4

# Stop services
docker-compose stop

# Clean restart
docker-compose down -v && docker-compose up -d
```

---

## ğŸ“Š Performance Characteristics

### Benchmarks (Real-World)

| Operation | Performance | Notes |
|-----------|------------|-------|
| **Search** | 45-80ms | Semantic search (10 results) |
| **Ingestion** | 1-2min | Per document (with GROBID) |
| **Embedding** | 1.12s | With 90% cache hit rate |
| **Download** | 5-15s | Per PDF from Sci-Hub |
| **Chunking** | 100-500ms | Depends on document size |

### Scalability

| Metric | Current | Tested | Limit |
|--------|---------|--------|-------|
| **Documents** | 26 | 100+ | Millions |
| **Chunks** | 1,228 | 5,000+ | Billions |
| **Workers** | 2 | 8 | Unlimited |
| **Cache Size** | 512MB | 2GB | 16GB+ |
| **QPS** | ~20 | 100+ | 1000+ |

---

## ğŸ” Security & Compliance

### Data Protection
- âœ… Read-only volume mounts (`:ro`)
- âœ… No hardcoded credentials
- âœ… Environment-based configuration
- âœ… Service isolation via Docker networks

### Sci-Hub Compliance
- âš ï¸ **Legal Notice**: Sci-Hub access may violate copyright laws
- âœ… Rate limiting to avoid service abuse
- âœ… User-agent identification
- âœ… Respectful retry patterns

### Privacy
- âœ… No external telemetry
- âœ… Local-first architecture
- âœ… Data stays in your infrastructure

---

## ğŸ› ï¸ Operational Tools

### Utility Scripts

1. **backfill_topics.py**: Add topics to existing documents
2. **ingest_gcs_prefix.py**: Bulk ingest from GCS
3. **run_full_ingestion.sh**: End-to-end automation

### Monitoring
- Prometheus-compatible metrics endpoint
- Structured JSON logging (Loguru)
- DLQ for error tracking
- Real-time job status API

### Debugging
```bash
# Check API health
curl http://localhost:8000/metrics

# View worker logs
docker-compose logs -f worker

# Check Qdrant directly
curl http://localhost:6333/collections/docs

# Redis inspection
docker exec -it vectorial_db-redis-1 redis-cli

# GROBID status
curl http://localhost:8070/api/isalive
```

---

## ğŸ“ˆ Use Cases

### 1. Literature Review
- Download 100+ papers from Sci-Hub
- Auto-ingest into vector DB
- Semantic search across all content
- Filter by year, topic, journal

### 2. Research Question Answering
- "What is the relationship between sleep and obesity?"
- Find relevant passages across entire corpus
- Rank by semantic similarity
- Extract key findings from conclusions

### 3. Meta-Analysis Preparation
- Collect papers on specific topic
- Extract methodology sections
- Find statistical results
- Identify study populations

### 4. Knowledge Base Construction
- Continuous paper ingestion
- Automatic topic classification
- Cross-document search
- Citation network analysis

---

## ğŸ“ Technical Deep Dives

### Embedding Strategy
**Model**: `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
- **Languages**: 50+ supported
- **Speed**: Fast inference (~10ms per text)
- **Quality**: Strong for biomedical text
- **Size**: 384 dimensions (compact)

### Chunking Algorithm
```python
1. Parse document with GROBID â†’ sections
2. For each section:
   a. Split into sentences
   b. Group sentences into chunks (~900 tokens)
   c. Add 150-token overlap between chunks
   d. Preserve section context in metadata
3. Return chunks with hierarchy preserved
```

### Search Algorithm
```python
1. User query â†’ FastEmbed â†’ 384-dim vector
2. Qdrant HNSW search:
   - Cosine similarity
   - Top-K results (default: 10)
   - Optional filters applied
3. Return ranked results with scores
```

---

## ğŸš§ Limitations & Known Issues

### Current Limitations
1. **Language**: Optimized for English (multilingual support exists)
2. **PDF Quality**: Scanned/image PDFs require OCR (not included)
3. **Large Files**: >50MB PDFs may timeout (configurable)
4. **Real-time**: Ingestion is async (not instant)

### Workarounds
1. **Scanned PDFs**: Pre-process with Tesseract OCR
2. **Timeouts**: Increase GROBID timeout in config
3. **Memory**: Scale Redis/workers for large batches

---

## ğŸ“š Documentation Structure

```
documentation/
â”œâ”€â”€ INDEX.md                   # Documentation hub
â”œâ”€â”€ QUICKSTART.md              # Getting started
â”œâ”€â”€ PlanDeImplementacion.md    # Full project plan
â”œâ”€â”€ FASE1_COMPLETED.md         # Phase 1 technical docs
â”œâ”€â”€ SCIHUB_DOWNLOADER.md       # Downloader guide
â”œâ”€â”€ APP_FUNCTIONALITY.md       # This file (complete overview)
â””â”€â”€ README.md                  # Documentation index
```

---

## ğŸ¯ Success Metrics

### Achieved
âœ… **100% of Phase 1 goals** completed  
âœ… **26 real papers** successfully processed  
âœ… **1,228 chunks** vectorized and searchable  
âœ… **90% cache hit** rate (target: 70%)  
âœ… **50ms average** search latency (target: <100ms)  
âœ… **100% test** coverage on critical paths  
âœ… **Zero data loss** with DLQ system  

---

## ğŸ”® Future Enhancements (Post-MVP)

### Planned Features
- [ ] OCR integration for scanned PDFs
- [ ] Multi-language UI
- [ ] GraphQL API
- [ ] WebSocket for real-time updates
- [ ] Citation network visualization
- [ ] Auto-summarization with LLMs
- [ ] Named entity recognition
- [ ] Automatic keyword extraction
- [ ] PDF annotation export
- [ ] Collaborative features

---

## ğŸ“ Getting Started

### Quick Start (5 minutes)
```bash
# 1. Clone repo
git clone <repo-url>
cd vectorial_db

# 2. Start services
docker-compose up -d

# 3. Wait for startup (30 seconds)
sleep 30

# 4. Download papers
python3 scripts/download_scihub_batch.py \
  --input scripts/example_dois.txt \
  --output downloads/papers

# 5. Ingest papers
python3 scripts/ingest_batch.py \
  --directory downloads/papers \
  --api-url http://localhost:8000

# 6. Search!
curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{"query": "your research question", "limit": 5}'
```

### Next Steps
1. Read [QUICKSTART.md](./QUICKSTART.md) for detailed examples
2. Review [INDEX.md](./INDEX.md) for all documentation
3. Check [SCIHUB_DOWNLOADER.md](./SCIHUB_DOWNLOADER.md) for advanced downloading
4. Explore API with `curl` or Postman

---

## ğŸ† Project Status

**Current State**: âœ… **Production-Ready Phase 1**

- âœ… Core functionality complete
- âœ… Comprehensive testing
- âœ… Full documentation
- âœ… Real-world validation
- âœ… Performance optimized
- âœ… Error handling robust
- âœ… Deployment automated

**Ready for**: Research, production use, scaling

---

**Last Updated**: October 3, 2025  
**Version**: 0.1.0  
**Status**: Production-Ready  
**Documentation**: Complete

